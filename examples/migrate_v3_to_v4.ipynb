{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648b1c03",
   "metadata": {},
   "source": [
    "# Dataset migration from v3 to Mnova ablation studies\n",
    "\n",
    "Going to remove 1d data and replace it with Mnova simulated data\n",
    "\n",
    "`v4`: Same data, just with Mnova 1d data replacing the existing train/val/test\n",
    "\n",
    "`v4_large`: Train on whole retrieval set with Mnova simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416be665",
   "metadata": {},
   "source": [
    "### Step 1: Cleaning Dataset for Mnova Simulation\n",
    "\n",
    "There's a lot of repeated SMILES strings, we want to simulate only once and also clear out anything with huge molecular weight (>1800 Da)\n",
    "\n",
    "Statistics:\n",
    "- 216,586 entries in train/val/test\n",
    "- 190,164 unique SMILES across train/val/test\n",
    "- 189,691 unique SMILES kept for simulation (<=1800 Da)\n",
    "- 182,637 unique SMILES kept for simulation (<=1000 Da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fbcaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def check_invalid_mol(smiles: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the molecule described by `smiles` has\n",
    "    molecular weight > 1800 Da or is invalid. Returns False otherwise.\n",
    "    \"\"\"\n",
    "    if not smiles:\n",
    "        return True\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return True\n",
    "\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    return mw > 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e4a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190164/190164 [00:18<00:00, 10074.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 182637/216586 valid SMILES to /data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv4/WorkingDir/smiles_1000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_ROOT = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3'\n",
    "OUTPUT_ROOT = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv4/WorkingDir'\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "index: dict[int, dict] = pickle.load(open(os.path.join(DATASET_ROOT, 'index.pkl'), 'rb'))\n",
    "all_smiles: list[str] = list(set([e['smiles'] for e in index.values()]))\n",
    "valid_smiles: list[str] = [s for s in tqdm(all_smiles) if not check_invalid_mol(s)]\n",
    "\n",
    "valid_smiles_path = os.path.join(OUTPUT_ROOT, 'smiles_1000.txt')\n",
    "with open(valid_smiles_path, 'w') as f:\n",
    "    f.write('\\n'.join(valid_smiles))\n",
    "\n",
    "print(f'Wrote {len(valid_smiles)}/{len(index)} valid SMILES to {valid_smiles_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c0cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_smiles = set(valid_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d91056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 230855/518901 [00:25<00:31, 9144.11it/s][15:13:27] WARNING: not removing hydrogen atom without neighbors\n",
      " 49%|████▉     | 253965/518901 [00:27<00:28, 9242.86it/s][15:13:30] WARNING: not removing hydrogen atom without neighbors\n",
      " 63%|██████▎   | 328716/518901 [00:36<00:20, 9187.10it/s][15:13:38] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|██████████| 518901/518901 [00:56<00:00, 9137.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 304294/518901 valid SMILES to /data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv4/WorkingDir/smiles_retrieval.txt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_ROOT = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3'\n",
    "OUTPUT_ROOT = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv4/WorkingDir'\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "index: dict[int, dict] = pickle.load(open(os.path.join(DATASET_ROOT, 'retrieval.pkl'), 'rb'))\n",
    "all_smiles: list[str] = list(set([e['smiles'] for e in index.values()]))\n",
    "valid_smiles: list[str] = [s for s in tqdm(all_smiles) if not check_invalid_mol(s)]\n",
    "valid_smiles = list(set(valid_smiles) - old_smiles)\n",
    "valid_smiles_path = os.path.join(OUTPUT_ROOT, 'smiles_retrieval.txt')\n",
    "with open(valid_smiles_path, 'w') as f:\n",
    "    f.write('\\n'.join(valid_smiles))\n",
    "\n",
    "print(f'Wrote {len(valid_smiles)}/{len(index)} valid SMILES to {valid_smiles_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da981b60",
   "metadata": {},
   "source": [
    "# JSONL Format for MoonshotDataset\n",
    "```json\n",
    "{\n",
    "    \"idx\": 0,\n",
    "    \"smiles\": \"C=CC1CN2CCC1CC2C(O)c1ccnc2ccc(OC)cc12\",\n",
    "    \"split\": \"train\",\n",
    "    \"has_hsqc\": true,\n",
    "    \"has_c_nmr\": false,\n",
    "    \"has_h_nmr\": false,\n",
    "    \"has_mass_spec\": true,\n",
    "    \"has_iso_dist\": true,\n",
    "    \"mw\": 404.1099,\n",
    "    \"name\": \"quinine hydrobromide\",\n",
    "    \"has_mw\": true,\n",
    "    \"formula\": \"C20H25BrN2O2\",\n",
    "    \"has_formula\": true,\n",
    "    \"np_pathway\": [\"Alkaloids\"],\n",
    "    \"np_superclass\": [\"Tryptophan alkaloids\"],\n",
    "    \"np_class\": [],\n",
    "    \"hsqc\": [\n",
    "        [54.89, 3.077, -1.0], \n",
    "        ...\n",
    "    ],\n",
    "    \"mass_spec\": [\n",
    "        [93.06987762451172, 1.3903098106384277], \n",
    "        ...\n",
    "    ],\n",
    "    \"c_nmr\": [\n",
    "        2.130000114440918, 2.25, ...\n",
    "    ],\n",
    "    \"h_nmr\": [\n",
    "        2.130000114440918, 2.25, ...\n",
    "    ],\n",
    "    \"fragidx\": [\n",
    "        1, 3, 4, 5, 6, 8, 9, 14, 15, 17, 19, 20, 29, 32, 35, 46, 50, 59, 60, 62, 69, 80, 81, 125, 184, 210, 240, 378, 382, 392, 472, 491, 698, 891, 933, 1439, 1486, 1639, 2185, 2792, 5479, 5610, 6182, 7318, 7903, 11697, 14346\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992e1e2",
   "metadata": {},
   "source": [
    "# JSONL Format for Predictions\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"idx\":0,\n",
    "    \"smiles\":\"COc1ccc2cc1Oc1cc(ccc1O)C(O)C13SSSC4(C(=O)N1C)C(O)C1=COC=CC(OC2=O)C1N4C3=O\",\n",
    "    \"status\":\"SUCCESS\",\n",
    "    \"error\":null,\n",
    "    \"atoms\":[\n",
    "        {\"number\":\"1\",\"name\":\"CH3\"},\n",
    "        {\"number\":\"2\",\"name\":\"O\"},\n",
    "        ...\n",
    "    ],\n",
    "    \"predictions\":{\n",
    "        \"hsqc\":{\n",
    "            \"status\":\"SUCCESS\",\n",
    "            \"H\":[\n",
    "                {\n",
    "                    \"atom\":[{\"index\":1}],\n",
    "                    \"shift\":{\"value\":3.9237684493895095,\"error\":0.1},\n",
    "                    \"js\":[{\"atom\":[{\"index\":1}],\"j\":{\"value\":5.46,\"error\":1.72}}]\n",
    "                },\n",
    "                {\n",
    "                    \"atom\":[{\"index\":4}],\n",
    "                    \"shift\":{\"value\":7.14351619175921,\"error\":0.1},\n",
    "                    \"js\":[{\"atom\":[{\"index\":5}],\"j\":{\"value\":8.65,\"error\":0.31}},{\"atom\":[{\"index\":7}],\"j\":{\"value\":0.1,\"error\":0.1}}]\n",
    "                },\n",
    "                {\n",
    "                    \"atom\":[{\"index\":5}],\n",
    "                    \"shift\":{\"value\":7.5815223902024345,\"error\":0.1},\n",
    "                    \"js\":[{\"atom\":[{\"index\":4}],\"j\":{\"value\":8.65,\"error\":0.31}},{\"atom\":[{\"index\":7}],\"j\":{\"value\":2.03,\"error\":0.48}}]\n",
    "                },\n",
    "                ...\n",
    "            ],\n",
    "            \"C\":[\n",
    "                {\n",
    "                    \"atom\":[{\"index\":1}],\n",
    "                    \"shift\":{\"value\":56.0195331969619,\"error\":3},\n",
    "                    \"js\":[{\"atom\":[{\"index\":1}],\"j\":{\"value\":143.96,\"error\":3.91}},{\"atom\":[{\"index\":4}],\"j\":{\"value\":0.4,\"error\":1.18}}]\n",
    "                },\n",
    "                {\n",
    "                    \"atom\":[{\"index\":3}],\n",
    "                    \"shift\":{\"value\":154.3666380106334,\"error\":3},\n",
    "                    \"js\":[{\"atom\":[{\"index\":1}],\"j\":{\"value\":3.97,\"error\":12.36}},{\"atom\":[{\"index\":4}],\"j\":{\"value\":2.52,\"error\":4.27}},{\"atom\":[{\"index\":5}],\"j\":{\"value\":7.17,\"error\":2.84}},{\"atom\":[{\"index\":7}],\"j\":{\"value\":6.43,\"error\":2.84}}]\n",
    "                },\n",
    "                ...\n",
    "            ],\n",
    "            \"error\":null\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0286be2",
   "metadata": {},
   "source": [
    "# Dataset Forms\n",
    "\n",
    "We will reduce the dataset to the following forms:\n",
    "\n",
    "Form 1\n",
    "\n",
    "- **MARINABase1**:\n",
    "    - MoonshotDatasetv3 without any molecules that errored during predictions and filter all molecules <= 1000 Da\n",
    "- **MARINADataset1**:\n",
    "    - MARINABase1, with replacing all existing C/H NMRs\n",
    "- **MARINADataset2**:\n",
    "    - MARINABase1, put all C/H simulated NMR that exist\n",
    "- **MARINADataset3**:\n",
    "    - MARINABase1, replacing all existing C/H/HSQC NMRs\n",
    "- **MARINADataset4**:\n",
    "    - MARINABase1, put all C/H/HSQC simulated NMR that exist\n",
    "\n",
    "Form 2\n",
    "\n",
    "- **MARINABase2**:\n",
    "    - MoonshotDatasetv3 without any molecules that errored during predictions and filter all molecules within [100Da, 1000Da]\n",
    "- **MARINAMedDataset1**:\n",
    "    - MARINABase2, with replacing all existing C/H NMRs\n",
    "- **MARINAMedDataset2**:\n",
    "    - MARINABase2, put all C/H simulated NMR that exist\n",
    "- **MARINAMedDataset3**:\n",
    "    - MARINABase2, replacing all existing C/H/HSQC NMRs\n",
    "- **MARINAMedDataset4**:\n",
    "    - MARINABase2, put all C/H/HSQC simulated NMR that exist\n",
    "\n",
    "Form 3\n",
    "\n",
    "- **MARINABaseNoDup**:\n",
    "    - MoonshotDatasetv3 without any molecules that errored during predictions and filter all molecules <= 1000 Da, and also no duplicate SMILES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20664e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def zip_directory(src_dir, output_zip_path):\n",
    "    src_dir = Path(src_dir)\n",
    "    output_zip_path = Path(output_zip_path)\n",
    "\n",
    "    with zipfile.ZipFile(output_zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in src_dir.rglob(\"*\"):\n",
    "            if file.is_file():\n",
    "                # keep relative paths inside the zip\n",
    "                zipf.write(file, arcname=file.relative_to(src_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5b7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [02:15<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "PRED_ROOT = '/data/nas-gpu/wang/atong/Datasets/MnovaPredictions/raw'\n",
    "files = sorted(glob.glob(os.path.join(PRED_ROOT, '*.jsonl')))\n",
    "def process_file(file):\n",
    "    nmrs = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            if data['predictions']['hsqc']['status'] != 'SUCCESS':\n",
    "                continue\n",
    "            h_nmr = data['predictions']['hsqc']['H']\n",
    "            c_nmr = data['predictions']['hsqc']['C']\n",
    "            if h_nmr is None or c_nmr is None or len(h_nmr) == 0 or len(c_nmr) == 0:\n",
    "                continue\n",
    "            nmrs[data['smiles']] = {\n",
    "                'h_nmr': h_nmr,\n",
    "                'c_nmr': c_nmr,\n",
    "                'atoms': data['atoms']\n",
    "            }\n",
    "    return nmrs\n",
    "\n",
    "all_nmrs = {}\n",
    "for file in tqdm(files):\n",
    "    nmrs = process_file(file)\n",
    "    all_nmrs.update(nmrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6bd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple\n",
    "nmr_data = {}\n",
    "\n",
    "def _atom_sign_from_name(atom_name: str) -> int:\n",
    "    return -1 if \"CH2\" in atom_name else +1\n",
    "\n",
    "def assemble_nmr_data(preds: Dict[str, Any]) -> Dict[str, List]:\n",
    "    data = {\n",
    "        \"h_nmr\": [],\n",
    "        \"c_nmr\": [],\n",
    "        \"hsqc\": [],\n",
    "        \"h_nmr_error\": [],\n",
    "        \"c_nmr_error\": [],\n",
    "        \"hsqc_error\": [],\n",
    "    }\n",
    "\n",
    "    atom_name_by_idx: Dict[int, str] = {}\n",
    "    for a in preds['atoms']:\n",
    "        idx = int(a[\"number\"])\n",
    "        atom_name_by_idx[idx] = a['name']\n",
    "\n",
    "    c_by_atom: Dict[int, Tuple[float, float]] = {}\n",
    "    for c in preds['c_nmr']:\n",
    "        for atom in c['atom']:\n",
    "            atom_idx = atom['index']\n",
    "            c_shift = float(c['shift']['value'])\n",
    "            c_err = float(c['shift']['error'])\n",
    "            c_by_atom[int(atom_idx)] = (c_shift, c_err)\n",
    "            data[\"c_nmr\"].append(c_shift)\n",
    "            data[\"c_nmr_error\"].append(c_err)\n",
    "\n",
    "    for h in preds['h_nmr']:\n",
    "        for atom in h['atom']:\n",
    "            atom_idx = atom['index']\n",
    "            h_shift = float(h['shift']['value'])\n",
    "            h_err = float(h['shift']['error'])\n",
    "            data[\"h_nmr\"].append(h_shift)\n",
    "            data[\"h_nmr_error\"].append(h_err)\n",
    "\n",
    "            if atom_idx not in c_by_atom:\n",
    "                if atom_name_by_idx[atom_idx] in ('CH', 'CH2', 'CH3'):\n",
    "                    raise ValueError()\n",
    "                continue\n",
    "            c_shift, c_err = c_by_atom[atom_idx]\n",
    "            sign = _atom_sign_from_name(atom_name_by_idx[atom_idx])\n",
    "\n",
    "            data[\"hsqc\"].append([c_shift, h_shift, sign])\n",
    "            data[\"hsqc_error\"].append([c_err, h_err, 0.0])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9697b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182619/182619 [00:31<00:00, 5831.40it/s] \n"
     ]
    }
   ],
   "source": [
    "for smiles, nmr in tqdm(all_nmrs.items()):\n",
    "    nmr_data[smiles] = assemble_nmr_data(nmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8848f",
   "metadata": {},
   "source": [
    "#### Build MARINAControl1\n",
    "\n",
    "Original MARINA training set, but filter out all molecules <1000 Da and not present in MARINABase1.\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINAControl1_jsonl ~/Datasets/MARINAControl1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06fc251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:04, 37967.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINAControl1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 41416.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINAControl1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 39576.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINAControl1_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINAControl1_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINAControl1_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': data['has_hsqc'] and len(data['hsqc']) > 0,\n",
    "                    'has_c_nmr': data['has_c_nmr'] and len(data['c_nmr']) > 0,\n",
    "                    'has_h_nmr': data['has_h_nmr'] and len(data['h_nmr']) > 0,\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "\n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196be318",
   "metadata": {},
   "source": [
    "#### Build MARINABase1\n",
    "\n",
    "Replace all H NMR, all C NMR, preserve HSQC NMR available. If nmr data does not exist for SMILES, delete SMILES.\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINABase1_jsonl ~/Datasets/MARINABase1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47a63c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:04, 36962.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINABase1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 39711.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINABase1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 38851.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINABase1_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINABase1_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINABase1_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                data['h_nmr'] = nmr_data[data['smiles']]['h_nmr']\n",
    "                data['c_nmr'] = nmr_data[data['smiles']]['c_nmr']\n",
    "                data['h_nmr_error'] = nmr_data[data['smiles']]['h_nmr_error']\n",
    "                data['c_nmr_error'] = nmr_data[data['smiles']]['c_nmr_error']\n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': data['has_hsqc'],\n",
    "                    'has_c_nmr': True,\n",
    "                    'has_h_nmr': True,\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "\n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da243521",
   "metadata": {},
   "source": [
    "#### Build MARINADataset1\n",
    "\n",
    "MARINABase1, but filter out all molecules <1000 Da and do not give C NMR and H NMR if not previously existed\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINADataset1_jsonl ~/Datasets/MARINADataset1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010ef3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "124044it [00:03, 43511.49it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:30, 5744.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 39155.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 38061.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset1_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINADataset1_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINADataset1_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                if data['has_h_nmr']:\n",
    "                    data['h_nmr'] = nmr_data[data['smiles']]['h_nmr']\n",
    "                    data['h_nmr_error'] = nmr_data[data['smiles']]['h_nmr_error']\n",
    "                if data['has_c_nmr']:\n",
    "                    data['c_nmr'] = nmr_data[data['smiles']]['c_nmr']\n",
    "                    data['c_nmr_error'] = nmr_data[data['smiles']]['c_nmr_error']\n",
    "                \n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': data['has_hsqc'],\n",
    "                    'has_c_nmr': data['has_c_nmr'],\n",
    "                    'has_h_nmr': data['has_h_nmr'],\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "    \n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb675d7",
   "metadata": {},
   "source": [
    "#### Build MARINADataset2\n",
    "\n",
    "MARINABase1, but filter out all molecules <1000 Da and give all C and H simulated\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINADataset2_jsonl ~/Datasets/MARINADataset2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab15b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:04, 37139.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset2_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 39011.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset2_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 37912.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset2_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINADataset2_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINADataset2_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                data['h_nmr'] = nmr_data[data['smiles']]['h_nmr']\n",
    "                data['c_nmr'] = nmr_data[data['smiles']]['c_nmr']\n",
    "                data['h_nmr_error'] = nmr_data[data['smiles']]['h_nmr_error']\n",
    "                data['c_nmr_error'] = nmr_data[data['smiles']]['c_nmr_error']\n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': data['has_hsqc'],\n",
    "                    'has_c_nmr': True,\n",
    "                    'has_h_nmr': True,\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "\n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc380ff",
   "metadata": {},
   "source": [
    "#### Build MARINADataset3\n",
    "\n",
    "MARINABase1, but filter out all molecules <1000 Da and replace all C and H and HSQC simulated that exist already\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINADataset3_jsonl ~/Datasets/MARINADataset3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48dd2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51650it [00:01, 33620.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:04, 38876.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset3_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 39347.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset3_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 38253.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset3_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINADataset3_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINADataset3_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                if data['has_h_nmr']:\n",
    "                    data['h_nmr'] = nmr_data[data['smiles']]['h_nmr']\n",
    "                    data['h_nmr_error'] = nmr_data[data['smiles']]['h_nmr_error']\n",
    "                if data['has_c_nmr']:\n",
    "                    data['c_nmr'] = nmr_data[data['smiles']]['c_nmr']\n",
    "                    data['c_nmr_error'] = nmr_data[data['smiles']]['c_nmr_error']\n",
    "                if data['has_hsqc']:\n",
    "                    data['hsqc'] = nmr_data[data['smiles']]['hsqc']\n",
    "                    data['hsqc_error'] = nmr_data[data['smiles']]['hsqc_error']\n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': data['has_hsqc'],\n",
    "                    'has_c_nmr': data['has_c_nmr'],\n",
    "                    'has_h_nmr': data['has_h_nmr'],\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "    \n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267e01d",
   "metadata": {},
   "source": [
    "#### Build MARINADataset4\n",
    "\n",
    "MARINABase1, but filter out all molecules <1000 Da and use all C and H and HSQC simulated data\n",
    "\n",
    "Then run\n",
    "```\n",
    "cd examples/scripts\n",
    "python convert.py --to-lmdb ~/Datasets/MARINADataset4_jsonl ~/Datasets/MARINADataset4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f50a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173422it [00:04, 36672.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 146375/146375 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21621it [00:00, 39077.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18228/18228 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21543it [00:00, 38116.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 18245/18245 molecules to /data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl/test.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl/RankingEntropy/rankingset.pt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_BASE = '/data/nas-gpu/wang/atong/Datasets/MoonshotDatasetv3_jsonl'\n",
    "OUT_BASE = '/data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl'\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def process_set(set_name: str, index: dict):\n",
    "    all_data = []\n",
    "    processed_smiles = set()\n",
    "    with open(os.path.join(v3_BASE, f'{set_name}.jsonl'), 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "            if data['smiles'] in nmr_data:\n",
    "                if data['smiles'] in processed_smiles:\n",
    "                    continue\n",
    "                processed_smiles.add(data['smiles'])\n",
    "                data['h_nmr'] = nmr_data[data['smiles']]['h_nmr']\n",
    "                data['h_nmr_error'] = nmr_data[data['smiles']]['h_nmr_error']\n",
    "                data['c_nmr'] = nmr_data[data['smiles']]['c_nmr']\n",
    "                data['c_nmr_error'] = nmr_data[data['smiles']]['c_nmr_error']\n",
    "                if len(nmr_data[data['smiles']]['hsqc']) > 0:\n",
    "                    data['hsqc'] = nmr_data[data['smiles']]['hsqc']\n",
    "                    data['hsqc_error'] = nmr_data[data['smiles']]['hsqc_error']\n",
    "                data['idx'] = len(index)\n",
    "                all_data.append(data)\n",
    "                index[len(index)] = {\n",
    "                    'smiles': data['smiles'],\n",
    "                    'name': data['name'],\n",
    "                    'formula': data['formula'],\n",
    "                    'np_pathway': data['np_pathway'],\n",
    "                    'np_superclass': data['np_superclass'],\n",
    "                    'np_class': data['np_class'],\n",
    "                    'has_hsqc': 'hsqc_error' in data,\n",
    "                    'has_c_nmr': True,\n",
    "                    'has_h_nmr': True,\n",
    "                    'has_mass_spec': data['has_mass_spec'],\n",
    "                    'has_mw': data['has_mw'],\n",
    "                    'has_formula': data['has_formula'],\n",
    "                    'mw': data['mw'],\n",
    "                    'split': set_name,\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(OUT_BASE, f'{set_name}.jsonl'), 'w') as f:\n",
    "        for data in all_data:\n",
    "            f.write(json.dumps(data) + '\\n')\n",
    "    print(f'Wrote {len(all_data)}/{len(processed_smiles)} molecules to {os.path.join(OUT_BASE, f\"{set_name}.jsonl\")}')    \n",
    "    return index\n",
    "    \n",
    "os.makedirs(os.path.join(OUT_BASE), exist_ok=True)\n",
    "index = {}\n",
    "index = process_set('train', index)\n",
    "index = process_set('val', index)\n",
    "index = process_set('test', index)\n",
    "pickle.dump(index, open(os.path.join(OUT_BASE, 'index.pkl'), 'wb'))\n",
    "\n",
    "shutil.copy(os.path.join(v3_BASE, 'metadata.json'), os.path.join(OUT_BASE, 'metadata.json'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'count_hashes_under_radius_6.pkl'), os.path.join(OUT_BASE, 'count_hashes_under_radius_6.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'retrieval.pkl'), os.path.join(OUT_BASE, 'retrieval.pkl'))\n",
    "os.makedirs(os.path.join(OUT_BASE, 'RankingEntropy'), exist_ok=True)\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'), os.path.join(OUT_BASE, 'RankingEntropy', 'bitinfo_to_idx.pkl'))\n",
    "shutil.copy(os.path.join(v3_BASE, 'RankingEntropy', 'rankingset.pt'), os.path.join(OUT_BASE, 'RankingEntropy', 'rankingset.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9ee52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINAControl1_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINAControl1\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINAControl1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 124546it [00:11, 14025.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:12, 11296.81it/s]\n",
      "val: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINAControl1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 14694.90it/s]\n",
      "test: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINAControl1_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 13835.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n",
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINABase1_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINABase1\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINABase1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:17, 8414.42it/s] \n",
      "val: 1024it [00:00, 8226.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINABase1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 11298.96it/s]\n",
      "test: 1024it [00:00, 8276.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINABase1_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 11029.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n",
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINADataset1_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINADataset1\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINADataset1_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:14, 9789.73it/s] \n",
      "val: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINADataset1_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 12936.59it/s]\n",
      "test: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINADataset1_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 13124.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n",
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINADataset2_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINADataset2\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINADataset2_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:16, 8614.50it/s] \n",
      "val: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINADataset2_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 11316.51it/s]\n",
      "test: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINADataset2_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 11286.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n",
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINADataset3_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINADataset3\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINADataset3_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:15, 9448.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINADataset3_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 12322.84it/s]\n",
      "test: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINADataset3_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 12548.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n",
      "Converting to LMDB format: /data/nas-gpu/wang/atong/Datasets//MARINADataset4_jsonl -> /data/nas-gpu/wang/atong/Datasets//MARINADataset4\n",
      "Copying regular files...\n",
      "  Copied index.pkl\n",
      "  Copied retrieval.pkl\n",
      "  Copied metadata.json\n",
      "  Copied count_hashes_under_radius_6.pkl\n",
      "  Copied RankingEntropy/\n",
      "\n",
      "Converting train from /data/nas-gpu/wang/atong/Datasets//MARINADataset4_jsonl/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 146375it [00:19, 7353.77it/s]\n",
      "val: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 146375 entries for train\n",
      "\n",
      "Converting val from /data/nas-gpu/wang/atong/Datasets//MARINADataset4_jsonl/val.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 18228it [00:01, 9714.39it/s] \n",
      "test: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18228 entries for val\n",
      "\n",
      "Converting test from /data/nas-gpu/wang/atong/Datasets//MARINADataset4_jsonl/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 18245it [00:01, 9673.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converted 18245 entries for test\n",
      "\n",
      "Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "scripts_dir = Path(\"/data/nas-gpu/wang/atong/SMART-Moonshot/examples/scripts\")\n",
    "DATASET_ROOT = '/data/nas-gpu/wang/atong/Datasets/'\n",
    "DATASETS = [\n",
    "    'MARINAControl1',\n",
    "    'MARINABase1',\n",
    "    'MARINADataset1',\n",
    "    'MARINADataset2',\n",
    "    'MARINADataset3',\n",
    "    'MARINADataset4'\n",
    "]\n",
    "for dataset in DATASETS:\n",
    "    cmd = [\n",
    "        \"python\",\n",
    "        \"convert.py\",\n",
    "        \"--to-lmdb\",\n",
    "        f'{DATASET_ROOT}/{dataset}_jsonl',\n",
    "        f'{DATASET_ROOT}/{dataset}',\n",
    "    ]\n",
    "    subprocess.run(cmd, cwd=scripts_dir, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b666f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:49<00:00, 48.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import shutil\n",
    "for dataset in tqdm(DATASETS):\n",
    "    zip_directory(os.path.join(DATASET_ROOT, dataset), os.path.join(DATASET_ROOT, f'{dataset}.zip'))\n",
    "    shutil.rmtree(os.path.join(DATASET_ROOT, f'{dataset}_jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf5c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 129014, 'smiles': 'Oc1c(Br)c(O)c(Br)c(O)c1Br', 'split': 'train', 'has_hsqc': False, 'has_c_nmr': True, 'has_h_nmr': True, 'has_mass_spec': True, 'has_iso_dist': True, 'mw': 362.799, 'name': 'tribromobenzene-1,3,5-triol', 'has_mw': True, 'formula': 'C6H3Br3O3', 'has_formula': True, 'np_pathway': ['Shikimates and Phenylpropanoids'], 'np_superclass': [], 'np_class': [], 'h_nmr': [7.1405471106596465, 7.1405471106596465, 7.1405471106596465], 'c_nmr': [153.33137424827768, 153.33137424827768, 153.33137424827768, 97.33674830695483, 97.33674830695483, 97.33674830695483], 'mass_spec': [[281.8521728515625, 15.783791542053223], [342.75994873046875, 3.930712938308716], [359.7626953125, 3.1665396690368652], [360.7705078125, 55.29762649536133]], 'fragidx': [6, 23, 29, 84, 437, 562, 4010], 'h_nmr_error': [3.83, 3.83, 3.83], 'c_nmr_error': [8.23, 8.23, 8.23, 9.6, 9.6, 9.6], 'hsqc': [], 'hsqc_error': []}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/data/nas-gpu/wang/atong/Datasets/MARINADataset4_jsonl/train.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        if data['idx'] == 129014:\n",
    "            print(data)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7ec65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h_nmr': [7.1405471106596465, 7.1405471106596465, 7.1405471106596465],\n",
       " 'c_nmr': [153.33137424827768,\n",
       "  153.33137424827768,\n",
       "  153.33137424827768,\n",
       "  97.33674830695483,\n",
       "  97.33674830695483,\n",
       "  97.33674830695483],\n",
       " 'hsqc': [],\n",
       " 'h_nmr_error': [3.83, 3.83, 3.83],\n",
       " 'c_nmr_error': [8.23, 8.23, 8.23, 9.6, 9.6, 9.6],\n",
       " 'hsqc_error': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmr_data['Oc1c(Br)c(O)c(Br)c(O)c1Br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01710a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
